{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette(\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_f_df = pd.read_csv(filepath_or_buffer='./data/train_fea.csv')\n",
    "cv_f_df = pd.read_csv(filepath_or_buffer='./data/cv_fea.csv')\n",
    "te_f_df = pd.read_csv(filepath_or_buffer='./data/test_fea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_cols = list(tr_f_df.columns)\n",
    "target = fea_cols.pop()\n",
    "labels = cv_f_df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tr_f_df[fea_cols].values\n",
    "y_train = tr_f_df[target].values\n",
    "\n",
    "X_cv = cv_f_df[fea_cols].values\n",
    "y_cv = cv_f_df[target].values\n",
    "\n",
    "X_test = te_f_df[fea_cols].values\n",
    "y_test = te_f_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, title, labels):\n",
    "    sns.heatmap(data=matrix, annot=True, fmt='.2f', linewidths=0.1,\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(xlabel='Predicted Class')\n",
    "    plt.ylabel(ylabel='Actual Class')\n",
    "    plt.title(label=title, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cmat = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)\n",
    "    pmat = cmat / cmat.sum(axis=0)\n",
    "    print(\"Column sum of precision matrix: {}\".format(pmat.sum(axis=0)))\n",
    "    rmat = ((cmat.T) / (cmat.sum(axis=1).T)).T\n",
    "    print(\"Row sum of recall matrix:       {}\".format(rmat.sum(axis=1)))\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.subplot(131)\n",
    "    plot_heatmap(matrix=cmat, title='Confusion Matrix', labels=labels)\n",
    "    plt.subplot(132)\n",
    "    plot_heatmap(matrix=pmat, title='Precision Matrix', labels=labels)\n",
    "    plt.subplot(133)\n",
    "    plot_heatmap(matrix=rmat, title='Recall Matrix', labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporter(clf, X, y, title, labels, best=None):\n",
    "    pred = clf.predict_proba(X=X)\n",
    "    \n",
    "    loss = log_loss(y_true=y, y_pred=pred)\n",
    "    loss = np.round(a=loss, decimals=3)\n",
    "    \n",
    "    cm_pred = clf.predict(X=X)\n",
    "    \n",
    "    print(title)\n",
    "    if best is None:\n",
    "        print(\"Logloss: {}\".format(loss))\n",
    "    else:\n",
    "        print(\"Logloss: {}\".format(loss))\n",
    "        print(\"Best parameters: {}\".format(best))\n",
    "    \n",
    "    plot_confusion_matrix(y_true=y, y_pred=cm_pred, labels=labels)\n",
    "    \n",
    "    print(classification_report(y_true=y, y_pred=cm_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner(clf, dist, X, y):\n",
    "    rs_clf = RandomizedSearchCV(estimator=clf, random_state=0, n_jobs=-1,\n",
    "                                param_distributions=dist)\n",
    "    search = rs_clf.fit(X=X, y=y)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_name):\n",
    "    if os.path.isdir('./model_dumps'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path='./model_dumps')\n",
    "    \n",
    "    model_path = os.path.join('./model_dumps', model_name)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_classifier(X_train,\n",
    "                     y_train,\n",
    "                     X_cv,\n",
    "                     y_cv,\n",
    "                     X_test,\n",
    "                     y_test,\n",
    "                     model_name,\n",
    "                     labels=labels):\n",
    "    model_path = get_model_path(model_name=model_name)\n",
    "    \n",
    "    if not os.path.isfile(path=model_path):\n",
    "        clf = DummyClassifier(strategy='uniform')\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "        \n",
    "        with open(file=model_path, mode='wb') as m_pkl:\n",
    "            pickle.dump(obj=clf, file=m_pkl)\n",
    "        print(\"Model saved into the disk.\\n\")\n",
    "    else:\n",
    "        with open(file=model_path, mode='rb') as m_pkl:\n",
    "            clf = pickle.load(file=m_pkl)\n",
    "        print(\"Loaded the saved model from the disk.\\n\")\n",
    "    \n",
    "    tr_loss = reporter(clf=clf, X=X_train, y=y_train,\n",
    "                       title='Train', labels=labels)\n",
    "    cv_loss = reporter(clf=clf, X=X_cv, y=y_cv,\n",
    "                       title='Cross Validation', labels=labels)\n",
    "    te_loss = reporter(clf=clf, X=X_test, y=y_test,\n",
    "                       title='Test', labels=labels)\n",
    "    \n",
    "    return tr_loss, cv_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_dummy_classifier.pkl'\n",
    "\n",
    "(dummy_tr_loss,\n",
    " dummy_cv_loss,\n",
    " dummy_te_loss) = dummy_classifier(X_train=X_train,\n",
    "                                   y_train=y_train,\n",
    "                                   X_cv=X_cv,\n",
    "                                   y_cv=y_cv,\n",
    "                                   X_test=X_test,\n",
    "                                   y_test=y_test,\n",
    "                                   model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regresson(X_train,\n",
    "                       y_train,\n",
    "                       X_cv,\n",
    "                       y_cv,\n",
    "                       X_test,\n",
    "                       y_test,\n",
    "                       dist,\n",
    "                       model_name,\n",
    "                       labels=labels):\n",
    "    model_path = get_model_path(model_name=model_name)\n",
    "\n",
    "    if not os.path.isfile(path=model_path):\n",
    "        clf = LogisticRegression(n_jobs=-1, random_state=42, max_iter=1000, \n",
    "                                 class_weight='balanced')\n",
    "\n",
    "        best = tuner(clf=clf, dist=dist, X=X_train, y=y_train)\n",
    "\n",
    "        clf = LogisticRegression(n_jobs=-1, max_iter=1000, C=best['C'],\n",
    "                                 random_state=42, penalty=best['penalty'],\n",
    "                                 class_weight='balanced')\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        sig_clf = CalibratedClassifierCV(base_estimator=clf)\n",
    "        sig_clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        with open(file=model_path, mode='wb') as m_pkl:\n",
    "            pickle.dump(obj=(clf, sig_clf, best), file=m_pkl)\n",
    "        print(\"Model saved into the disk.\\n\")\n",
    "    else:\n",
    "        with open(file=model_path, mode='rb') as m_pkl:\n",
    "            clf, sig_clf, best = pickle.load(file=m_pkl)\n",
    "        print(\"Loaded the saved model from the disk.\\n\")\n",
    "    \n",
    "    tr_loss = reporter(clf=sig_clf, X=X_train, y=y_train,\n",
    "                       title='Train', best=best, labels=labels)\n",
    "    cv_loss = reporter(clf=sig_clf, X=X_cv, y=y_cv,\n",
    "                       title='Cross Validation', best=best, labels=labels)\n",
    "    te_loss = reporter(clf=sig_clf, X=X_test, y=y_test,\n",
    "                       title='Test', best=best, labels=labels)\n",
    "    \n",
    "    return best, tr_loss, cv_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_logistic_regression.pkl'\n",
    "\n",
    "dist = dict(C=[10 ** x for x in range(-4, 3)], penalty=['l2', 'l1'])\n",
    "\n",
    "(logreg_best,\n",
    " logreg_tr_loss,\n",
    " logreg_cv_loss,\n",
    " logreg_te_loss) = logistic_regresson(X_train=X_train,\n",
    "                                      y_train=y_train,\n",
    "                                      X_cv=X_cv,\n",
    "                                      y_cv=y_cv,\n",
    "                                      X_test=X_test,\n",
    "                                      y_test=y_test,\n",
    "                                      dist=dist,\n",
    "                                      model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_classifier(X_train,\n",
    "                              y_train,\n",
    "                              X_cv,\n",
    "                              y_cv,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              dist,\n",
    "                              model_name,\n",
    "                              labels=labels):\n",
    "    model_path = get_model_path(model_name=model_name)\n",
    "\n",
    "    if not os.path.isfile(path=model_path):\n",
    "        clf = SVC(random_state=42, class_weight='balanced')\n",
    "\n",
    "        best = tuner(clf=clf, dist=dist, X=X_train, y=y_train)\n",
    "\n",
    "        clf = SVC(C=best['C'], random_state=42, class_weight='balanced')\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        sig_clf = CalibratedClassifierCV(base_estimator=clf)\n",
    "        sig_clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        with open(file=model_path, mode='wb') as m_pkl:\n",
    "            pickle.dump(obj=(clf, sig_clf, best), file=m_pkl)\n",
    "        print(\"Model saved into the disk.\\n\")\n",
    "    else:\n",
    "        with open(file=model_path, mode='rb') as m_pkl:\n",
    "            clf, sig_clf, best = pickle.load(file=m_pkl)\n",
    "        print(\"Loaded the saved model from the disk.\\n\")\n",
    "    \n",
    "    tr_loss = reporter(clf=sig_clf, X=X_train, y=y_train,\n",
    "                       title='Train', best=best, labels=labels)\n",
    "    cv_loss = reporter(clf=sig_clf, X=X_cv, y=y_cv,\n",
    "                       title='Cross Validation', best=best, labels=labels)\n",
    "    te_loss = reporter(clf=sig_clf, X=X_test, y=y_test,\n",
    "                       title='Test', best=best, labels=labels)\n",
    "    \n",
    "    return best, tr_loss, cv_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_support_vector_classifier.pkl'\n",
    "\n",
    "dist = dict(C=[10 ** x for x in range(-4, 3)])\n",
    "\"\"\"\n",
    "(svc_best,\n",
    " svc_tr_loss,\n",
    " svc_cv_loss,\n",
    " svc_te_loss) = support_vector_classifier(X_train=X_train,\n",
    "                                          y_train=y_train,\n",
    "                                          X_cv=X_cv,\n",
    "                                          y_cv=y_cv,\n",
    "                                          X_test=X_test,\n",
    "                                          y_test=y_test,\n",
    "                                          dist=dist,\n",
    "                                          model_name=model_name)\n",
    "\"\"\"\n",
    "# svc is broken, so skipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbors_classifier(X_train,\n",
    "                           y_train,\n",
    "                           X_cv,\n",
    "                           y_cv,\n",
    "                           X_test,\n",
    "                           y_test,\n",
    "                           dist,\n",
    "                           model_name,\n",
    "                           labels=labels):\n",
    "    model_path = get_model_path(model_name=model_name)\n",
    "\n",
    "    if not os.path.isfile(path=model_path):\n",
    "        clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "        best = tuner(clf=clf, dist=dist, X=X_train, y=y_train)\n",
    "\n",
    "        clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=best['n_neighbors'])\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        sig_clf = CalibratedClassifierCV(base_estimator=clf)\n",
    "        sig_clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        with open(file=model_path, mode='wb') as m_pkl:\n",
    "            pickle.dump(obj=(clf, sig_clf, best), file=m_pkl)\n",
    "        print(\"Model saved into the disk.\\n\")\n",
    "    else:\n",
    "        with open(file=model_path, mode='rb') as m_pkl:\n",
    "            clf, sig_clf, best = pickle.load(file=m_pkl)\n",
    "        print(\"Loaded the saved model from the disk.\\n\")\n",
    "    \n",
    "    tr_loss = reporter(clf=sig_clf, X=X_train, y=y_train,\n",
    "                       title='Train', best=best, labels=labels)\n",
    "    cv_loss = reporter(clf=sig_clf, X=X_cv, y=y_cv,\n",
    "                       title='Cross Validation', best=best, labels=labels)\n",
    "    te_loss = reporter(clf=sig_clf, X=X_test, y=y_test,\n",
    "                       title='Test', best=best, labels=labels)\n",
    "    \n",
    "    return best, tr_loss, cv_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'model_k_neighbors_classifier.pkl'\n",
    "\n",
    "dist = dict(n_neighbors=[3, 5, 11, 15, 21, 31, 41, 51, 99])\n",
    "\n",
    "(knn_best,\n",
    " knn_tr_loss,\n",
    " knn_cv_loss,\n",
    " knn_te_loss) = k_neighbors_classifier(X_train=X_train,\n",
    "                                       y_train=y_train,\n",
    "                                       X_cv=X_cv,\n",
    "                                       y_cv=y_cv,\n",
    "                                       X_test=X_test,\n",
    "                                       y_test=y_test,\n",
    "                                       dist=dist,\n",
    "                                       model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_plot(data, x, y, title):\n",
    "    bars = sns.barplot(data=data, x=x, y=y)\n",
    "    for b in bars.patches:\n",
    "        x = b.get_x() + (b.get_width() / 2)\n",
    "        y = np.round(b.get_height(), 3)\n",
    "        bars.annotate(text=format(y),\n",
    "                      xy=(x, y), ha='center', va='center', size=8, \n",
    "                      xytext=(0, 6), textcoords='offset points')\n",
    "    plt.title(label=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do other feature importance stuff\n",
    "fi_cols = ['redshift', 'g-r', 'i-z', 'u-r', 'i-r', 'z-r', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_tr_data = tr_f_df[fi_cols]\n",
    "fi_cv_data = cv_f_df[fi_cols]\n",
    "fi_te_data = te_f_df[fi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_data(data, target_arr, filename):\n",
    "    if os.path.isdir('./data/fi_data'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path='./data/fi_data')\n",
    "    \n",
    "    data['class'] = target_arr\n",
    "    data.to_csv(path_or_buf=os.path.join('./data/fi_data', filename), index=None)\n",
    "    print(\"The data is exported to '{}'.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data(data=fi_tr_data, target_arr=y_train, filename='fi_tr_data.csv')\n",
    "export_data(data=fi_cv_data, target_arr=y_cv, filename='fi_cv_data.csv')\n",
    "export_data(data=fi_te_data, target_arr=y_test, filename='fi_te_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(estimator=model,\n",
    "                             X=X, y=y, scoring='accuracy',\n",
    "                             cv=cv, n_jobs=-1,\n",
    "                             error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_classifier(X_train,\n",
    "                        y_train,\n",
    "                        X_cv,\n",
    "                        y_cv,\n",
    "                        X_test,\n",
    "                        y_test,\n",
    "                        models,\n",
    "                        model_name,\n",
    "                        labels=labels):\n",
    "    model_path = get_model_path(model_name=model_name)\n",
    "    \n",
    "    if not os.path.isfile(path=model_path):\n",
    "        clf = StackingClassifier(estimators=models)\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        with open(file=model_path, mode='wb') as m_pkl:\n",
    "            pickle.dump(obj=clf, file=m_pkl)\n",
    "        print(\"Model saved into the disk.\\n\")\n",
    "    else:\n",
    "        with open(file=model_path, mode='rb') as m_pkl:\n",
    "            clf = pickle.load(file=m_pkl)\n",
    "        print(\"Loaded the saved model from the disk.\\n\")\n",
    "    \n",
    "    tr_loss = reporter(clf=clf, X=X_train, y=y_train,\n",
    "                       title='Train', labels=labels)\n",
    "    cv_loss = reporter(clf=clf, X=X_cv, y=y_cv,\n",
    "                       title='Cross Validation', labels=labels)\n",
    "    te_loss = reporter(clf=clf, X=X_test, y=y_test,\n",
    "                       title='Test', labels=labels)\n",
    "    \n",
    "    return tr_loss, cv_loss, te_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_stacking_classifier.pkl'\n",
    "\n",
    "LR = LogisticRegression(penalty=logreg_best['penalty'],\n",
    "                        C=logreg_best['C'],\n",
    "                        class_weight='balanced',\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1, max_iter=1000)\n",
    "\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=knn_best['n_neighbors'], n_jobs=-1)\n",
    "\n",
    "models = [('LR', LR), ('KNN', KNN)]\n",
    "\n",
    "(stack_tr_loss,\n",
    " stack_cv_loss,\n",
    " stack_te_loss) = stacking_classifier(X_train=X_train,\n",
    "                                      y_train=y_train,\n",
    "                                      X_cv=X_cv,\n",
    "                                      y_cv=y_cv,\n",
    "                                      X_test=X_test,\n",
    "                                      y_test=y_test,\n",
    "                                      models=models,\n",
    "                                      model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Logistic Regression',\n",
    "               'K-Nearest Neighbors', 'Stacking Classifier']\n",
    "\n",
    "tr_losses = [logreg_tr_loss, knn_tr_loss, stack_tr_loss]\n",
    "cv_losses = [logreg_cv_loss, knn_cv_loss, stack_cv_loss]\n",
    "te_losses = [logreg_te_loss, knn_te_loss, stack_te_loss]\n",
    "\n",
    "summary_df = pd.DataFrame()\n",
    "summary_df['Models'] = model_names\n",
    "summary_df['Train Loss'] = tr_losses\n",
    "summary_df['CV Loss'] = cv_losses\n",
    "summary_df['Test Loss'] = te_losses\n",
    "\n",
    "summary = tabulate(tabular_data=summary_df, headers='keys',\n",
    "                   tablefmt='psql')\n",
    "print(summary)\n",
    "\n",
    "tidy = summary_df.melt(id_vars='Models').rename(columns=str.title)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=tidy, x='Models', y='Value', hue='Variable', alpha=0.9)\n",
    "plt.title(label='Logloss Obtained')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
